npca <- glm(diagnosis ~ ., subset)
npca <- lm(diagnosis ~ ., subset)
npca
npca <- train(diagnosis ~ ., method="glm", data=subset)
pca <- train(diagnosis ~., method="glm", preProcess="pca", data=subset)
testset <- testing[,grep("^IL|diagnosis$", colnames(testing))]
testset
pca
npca
npca <- preProcess(subset)
npcaPR <- predict(npca)
npcaPR
npca
predict(npca)
predict(pca)
npca <- train(diagnosis ~ ., method="glm", data=subset)
pca <- train(diagnosis ~ ., method="glm", method="pca", trControl=trainControl(preProcOptions=list(thresh=0.8)), data=subset)
pca <- train(diagnosis ~ ., method="glm", preProcess="pca", trControl=trainControl(preProcOptions=list(thresh=0.8)), data=subset)
confusionMatrix(testing$diagnosis,precit(npca,testing))
confusionMatrix(testing$diagnosis,predict(npca,testing))
confusionMatrix(testing$diagnosis,predict(pca,testing))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer)
min(training$Superplasticizer)
log(min(training$Superplasticizer))
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
testIndex
nrow(adData)
nrow(testIndex)
training = adData[-testIndex,]
testing = adData[testIndex,]
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
train = createDataPartition(diagnosis,p=0.5,list=F)
train
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
testing
training
training$VEGF = testing$VEGF
dupsBetweenGroups <- function (df, idcol) {
# df: the data frame
# idcol: the column which identifies the group each row belongs to
# Get the data columns to use for finding matches
datacols <- setdiff(names(df), idcol)
# Sort by idcol, then datacols. Save order so we can undo the sorting later.
sortorder <- do.call(order, df)
df <- df[sortorder,]
# Find duplicates within each id group (first copy not marked)
dupWithin <- duplicated(df)
# With duplicates within each group filtered out, find duplicates between groups.
# Need to scan up and down with duplicated() because first copy is not marked.
dupBetween = rep(NA, nrow(df))
dupBetween[!dupWithin] <- duplicated(df[!dupWithin,datacols])
dupBetween[!dupWithin] <- duplicated(df[!dupWithin,datacols], fromLast=TRUE) | dupBetween[!dupWithin]
# ============= Replace NA's with previous non-NA value ==============
# This is why we sorted earlier - it was necessary to do this part efficiently
# Get indexes of non-NA's
goodIdx <- !is.na(dupBetween)
# These are the non-NA values from x only
# Add a leading NA for later use when we index into this vector
goodVals <- c(NA, dupBetween[goodIdx])
# Fill the indices of the output vector with the indices pulled from
# these offsets of goodVals. Add 1 to avoid indexing to zero.
fillIdx <- cumsum(goodIdx)+1
# The original vector, now with gaps filled
dupBetween <- goodVals[fillIdx]
# Undo the original sort
dupBetween[sortorder] <- dupBetween
# Return the vector of which entries are duplicated across groups
return(dupBetween)
}
dupsBetweenGroups(testing, training)
library(AppliedPredictiveModeling)data(AlzheimerDisease)
library(AppliedPredictiveModeling)data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=F)
library(caret)
trainIndex = createDataPartition(diagnosis,p=0.5,list=F)
trainIndex
training = adData[trainIndex,]
testing = adData[-trainIndex,]
train2 = adData[trainIndex,]
test2 = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
train1 = adData[-testIndex,]
test1 = adData[testIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
subset <- training[,grep("^IL", colnames(training))]
preProcess(subset, method="pca", thres=0.8)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
View(segmentationOriginal)
tss <- createDataPartition( y=segmentationOriginal$Case, p=0.7, list=F )
training <- segmentationOriginal[tss,]
testing <- segmentationOriginal[-tss,]
set.seed(125)
training <- segmentationOriginal[Case=='Train',]
training <- segmentationOriginal[segmentationOriginal$Case=='Train',]
testing <- segmentationOriginal[segmentationOriginal$Case=='Test',]
fit <- train( class ~ ., method='rpart', data=training )
training
fit <- train( class ~ ., method='rpart', data=training )
str(training)
data(iris)
tss <- createDataPartition( y=iris$Species, p=0.7, list=F )
trset <- iris[tss,]
ttset <- iris[-tss,]
fit <- train( Species ~ ., method="rpart", data=trset )
str(trset)
fit <- train( Class ~ ., method="rpart", data=training )
?predict
fit
fit$finalModel
?"AppliedPredictiveModeling"
?caret
?"caret"
version(caret)
library(pgmm)
install.packages('pgmm')
library(pgmm)
data(olive)
olive = olive[,-1]
ofit <- train( Area ~., method="rpart", data=olive )
ofit
ofit$perfNames
ofit$finalModel
?tree
??tree
newdata = as.data.frame(t(colMeans(olive)))
tree(newdata)
newdata
?predict
predict(ofit, newdata=newdata)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
1:dim(SAheart)[1]
1:dim(SAheart)
set.seed(13234)
colnames(trainSA)
fitSA <- train( chd ~ age + alcohol + typea + ldl, method="glm", family="binomial", data=trainSA )
fitSA
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass
predict(fitSA)
predict(fitSA,newdata=testSA)
missCLass(trainSA,predict(fitSA))
missClass(trainSA,predict(fitSA))
missClass(trainSA,predict(fitSA,newdata=trainSA))
colnames(trainSA)
missClass(trainSA)
predict(trainSA)
predict(fitSA)
predict(fitSA)[1]
predict(fitSA) > 0.5
(predict(fitSA) > 0.5) * 1
sum((predict(fitSA) > 0.5) * 1)
sum((predict(fitSA) > 0.5) * 1) / length(predict(fitSA))
sum((predict(fitSA,newdata=testSA) > 0.5) * 1) / length(predict(fitSA,newdata=testSA))
sum((predict(fitSA) > 0.5) * 1) != predict(fitSA)
((predict(fitSA) > 0.5) * 1) != predict(fitSA)
sum((predict(fitSA,newdata=testSA) > 0.5) * 1) / length(predict(fitSA,newdata=testSA))
sum((predict(fitSA) > 0.5) * 1) / length(predict(fitSA))
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
vfit <- train( y ~ ., method="rf", data=vowel.train )
vfit <- train( y ~ ., method="rf", data=vowel.train )
varImp()
varImp(vfit)
library(ElemStatLearn)data(SAheart)set.seed(8484)train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)trainSA = SAheart[train,]testSA = SAheart[-train,]
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
?glm
fitSA <- glmn( chd ~ age + alcohol + obesity + typea + ldl, family="binomial", data=trainSA )
fitSA <- glm( chd ~ age + alcohol + obesity + typea + ldl, family="binomial", data=trainSA )
fitSA
summary(fitSa)
summary(fitSA)
predict(fitSA)
predict(fitSA) > 0.5
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass
fitSA$fitted.values
(predict(fitSA) > 0.5) * 1
sum((predict(fitSA) > 0.5) * 1)
sum((predict(fitSA) > 0.5) * 1) / length(trainSA)
sum((predict(fitSA) > 0.5) * 1) / length(trainSA$chd)
((predict(fitSA) > 0.5) * 1) != trainSA$chd
sum((predict(fitSA) > 0.5) * 1) != trainSA$chd)
sum((predict(fitSA) > 0.5) * 1) != trainSA$chd))
predict(fitSA) > 0.5
(predict(fitSA) > 0.5) * 1
(predict(fitSA) > 0.5) * 1 != trainSA$chd
sum((predict(fitSA) > 0.5) * 1 != trainSA$chd)
sum((predict(fitSA) > 0.5) * 1 != trainSA$chd) / length(trainSA$chd)
sum((predict(fitSA, newdata=testSA) > 0.5) * 1 != testSA$chd) / length(testSA$chd)
missClass(trainSA$chd,fitSA)
missClass(trainSA,fitSA)
missClass(trainSA$chd,predict(fitSA))
missClass(trainSA$chd,predict(fitSA,newdata=testSA))
missClass(testSA$chd,predict(fitSA,newdata=testSA))
missClass(trainSA$chd,predict(fitSA,type=response))
missClass(trainSA$chd,predict(fitSA,type='response'))
missClass(testSA$chd,predict(fitSA,newdata=testSA,type='response'))
load("~/R/PROJECTS/predicting-correct-exercises/workspace.RData")
tst
str(tst)
str(testing)
rft$finalModel
fit <- train( classe ~ . - X - user_name, method="glm", preProcess="pca", data=validation)
library(caret)
fit <- train( classe ~ . - X - user_name, method="glm", preProcess="pca", data=validation)
fit <- train( classe ~ . - X - user_name, method="gbm", preProcess="pca", data=validation)
fit <- train( classe ~ . - X - user_name, method="gbm", data=validation)
m <- abs(cor(validation[,-classe]))
m <- abs(cor(validation[,-c('X','user_name','classe')]))
m <- abs(cor(validation[,!colnames(validation) %in% c('X','user_name','classe')]))
for(n in names(validation)) { ratio <- how_na(validation,n); if(ratio > 0) { print(paste(n, ": ", sprintf("%.2f",ratio))) } }
for(n in names(validation)) { ratio <- how_na(validation,n); if(ratio > 0.9) { print(paste(n, ": ", sprintf("%.2f",ratio))) } }
for(n in names(validation)) { ratio <- how_na(validation,n); if(ratio > 0.9) { print(n) } }
whee <- list()
whee
mostly_na <- function(df,thresh=0.95) {
most_na_list <- list()
for(vn in names(df1)) {
na_ratio <- how_na(df,vn)
if (na_ratio >= thresh){
most_na_list[[length(most_na_list)+1]] <- vn
}
}
most_na_list
}
most_na_list(validation)
mostly_na(validation)
mostly_na <- function(df,thresh=0.95) {
most_na_list <- list()
for(vn in names(df)) {
na_ratio <- how_na(df,vn)
if (na_ratio >= thresh){
most_na_list[[length(most_na_list)+1]] <- vn
}
}
most_na_list
}
mostly_na(validation)
as.list(mostly_na(validation))
c(mostly_na(validation))
cbind(mostly_na(validation))
rbind(mostly_na(validation))
rbind(1,2)
c(1,2)
c(c(1,2),3)
mostly_na <- function(df,thresh=0.95) {
most_na_list <- list()
for(vn in names(df)) {
na_ratio <- how_na(df,vn)
if (na_ratio >= thresh){
c(most_na_list,vn)
}
}
most_na_list
}
mostly_na(validation)
mostly_na <- function(df,thresh=0.95) {
most_na_list <- c()
for(vn in names(df)) {
na_ratio <- how_na(df,vn)
if (na_ratio >= thresh){
c(most_na_list,vn)
}
}
most_na_list
}
mostly_na(validation)
a <- c()
c(a,'1')
a <- c(a,1)
a <- c(a,2)
a
mostly_na <- function(df,thresh=0.9) {
na_list <- c()
for(vn in names(df)) {
na_ratio <- how_na(df,vn)
print(paste(vn,na_ratio))
if (na_ratio >= thresh){
c(na_list,vn)
}
}
most_na_list
}
mostly_na(validation)
mostly_na <- function(df,thresh=0.9) {
na_list <- c()
for(vn in names(df)) {
na_ratio <- how_na(df,vn)
if (na_ratio >= thresh){
c(na_list,vn)
print(paste(vn,"threshold surpassed at",na_ratio))
}
}
na_list
}
mostly_na(validation)
mostly_na <- function(df,thresh=0.9) {
na_list <- c()
for(vn in names(df)) {
na_ratio <- how_na(df,vn)
if (na_ratio >= thresh){
na_list <- c(na_list,vn)
print(paste(vn,"threshold surpassed at",na_ratio))
}
}
na_list
}
mostly_na(validation)
prc <- prcomp(validation[,!colnames(validation) %in% mostly_na(validation)])
ignore <- mostly_na(validation)
fit <- train(classe ~ -ignore, validation, method="rf")
paste(ignore,collapse="-")
paste(ignore,collapse=" - ")
paste("classe ~ . -",paste(ignore,collapse=" - ")
)
as.formula(paste("classe ~ . -",paste(ignore,collapse=" - ")))
simpler_formula <- as.formula(paste("classe ~ . -", paste(mostly_na(validation),collapse=" - ")))
simpler_formula
fit <- train(simpler_formula, validation, method="pca")
fit <- train(simpler_formula, validation, method="glm")
fit <- train(simpler_formula, validation, method="rf")
fit
fit$finalModel
predict(fit,testing)
fit$pred
fit$bestTune
fit$coefnames
save.image("~/R/PROJECTS/predicting-correct-exercises/workspace.RData")
version
save.image("~/R/PROJECTS/predicting-correct-exercises/workspace.RData")
predict(fit,validation)
predict(fit,training)
2. Write using R Markdown
library(slidify)
library(slidifyLibraries)
2. Write using R Markdown
slidify("index.Rmd")
setwd("~/R/PROJECTS/per-square-foot")
slidify("index.Rmd")
setwd("~/R/PROJECTS/per-square-foot")
setwd("~/R/PROJECTS/per-square-foot")
* B
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
print(M1, tag = 'chart')
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
psf <- read.csv("data/psf_by_state.csv")
require(ggplot2)
summary(psf)
keep_these <- c('FIPS','Year','MeanPrice')
n <- melt(psf, id.vars=do_not_melt, variable.name="Year", value.name="MeanPrice")
library(dplyr)
n <- melt(psf, id.vars=do_not_melt, variable.name="Year", value.name="MeanPrice")
library(reshape2)
n <- melt(psf, id.vars=do_not_melt, variable.name="Year", value.name="MeanPrice")
do_not_melt <- c('RegionID','RegionName','State','Metro','StateCodeFIPS','MunicipalCodeFIPS','SizeRank')
n <- melt(psf, id.vars=do_not_melt, variable.name="Year", value.name="MeanPrice")
colnames(psf)
do_not_melt <- c('RegionID','RegionName','SizeRank')
n <- melt(psf, id.vars=do_not_melt, variable.name="Date", value.name="MeanPrice")
n
n$Year <- as.Date( paste(n$Year,'01'), 'X%Y.%m %d' )
summary(n)
psf <- read.csv("data/psf_by_state.csv")
summary(psf)
n <- psf
do_not_melt <- c('RegionID','RegionName','SizeRank')
n <- melt(n, id.vars=do_not_melt, variable.name="Date", value.name="MeanPrice")
head(n)
n$Date <- as.Date( paste(n$Date,'01'), 'X%Y.%m %d' )
head(n)
p <- ggplot(n, aes(Date, MeanPrice))
p + geom_line() + scale_x_date(format='%b-%Y')
p + geom_line()
p + geom_line(aes(color=RegionName))
m <- subset(m, RegionName %in% c("District of Columbia","Virginia"))
m <- subset(n, RegionName %in% c("District of Columbia","Virginia"))
p <- ggplot(m, aes(Date, MeanPrice))
p + geom_line(aes(color=RegionName))
m <- subset(n, RegionName %in% c("District of Columbia","Virginia","Maine"))
p <- ggplot(m, aes(Date, MeanPrice))
p + geom_line(aes(color=RegionName))
m <- subset(n, RegionName %in% c("District of Columbia","Virginia","Maine","Maryland"))
p <- ggplot(m, aes(Date, MeanPrice))
p + geom_line(aes(color=RegionName))
p  + geom_line(aes(color=RegionName)) + xlab("") + ylab("Mean Price per Sq.Ft")
m
p  + geom_line(aes(color=RegionName)) + xlab("") + ylab("Mean Price per Sq.Ft")
m  <- rename(psf, State=RegionID)
m
summary(m)
n <- rename(psf, State=RegionID)
psf <- read.csv("data/psf_by_state.csv")
keep_these <- c('FIPS','Year','MeanPrice')
do_not_melt <- c('RegionID','RegionName','SizeRank')
psf <- melt(psf, id.vars=do_not_melt, variable.name="Date", value.name="MeanPrice")
rm(n)
rm(m)
rm(p)
psf$Date <- as.Date( paste(psf$Date,'01'), 'X%Y.%m %d' )
colnames(psf)
colnames(psf)[2]
colnames(psf)[2] <- 'State'
colnames(psf)
keep_these <- c('State','Date','MeanPrice')
psf <- psf[,keep_these]
colnames(psf)
head(psf)
my_states <- subset(psf, RegionName %in% c("District of Columbia","Virginia","Maine","Maryland"))
my_states <- subset(psf, State %in% c("District of Columbia","Virginia","Maine","Maryland"))
psf_plot <- ggplot(my_states, aes(Date, MeanPrice))
psf_plot + geom_line(aes(color=RegionName)) + xlab("") + ylab("Mean Price per Sq.Ft in Dollars")
psf_plot + geom_line(aes(color=State)) + xlab("") + ylab("Mean Price per Sq.Ft in Dollars")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
